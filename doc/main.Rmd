---
title: "Airbnb Listing Optimization Analysis"
author: "Elena Dubova, Max Karsok, Matthew Vitha"
date: "May 1st, 2019"
output: html_document
---

# Step 1: Load required libraries and sources.

```{r libraries, echo = FALSE, warning=FALSE, message=FALSE}

if(!require("dplyr")){
  install.packages("dplyr")}
  library(dplyr)
if(!require("Hmisc")){
  install.packages("Hmisc")}
  library(Hmisc)
if(!require("qdap")){
  install.packages("qdap")}
  library(qdap)
if(!require("magrittr")){
  install.packages("magrittr")}
  library(magrittr)
if(!require("tm")){
  install.packages("tm")}
  library(tm)
if(!require("tidyverse")){
  install.packages("tidyverse")}
  library(tidyverse)
if(!require("tidytext")){
  install.packages("tidytext")}
  library(tidytext)
if(!require("ggplot2")){
  install.packages("ggplot2")}
  library(ggplot2)
if(!require("data.table")){
  install.packages("data.table")}
  library(data.table)
if(!require("wordcloud")){
  install.packages("wordcloud")}
  library(wordcloud)
if(!require("lubridate")){
  install.packages("lubridate")}
  library(lubridate)
if(!require("DT")){
  install.packages("DT")}
  library(DT)
if(!require("stringr")){
  install.packages("stringr")}
  library(stringr)
if(!require("topicmodels")){
  install.packages("topicmodels")}
  library(topicmodels)
if(!require("SnowballC")){
  install.packages("SnowballC")}
  library(SnowballC)
if(!require("classInt")){
  install.packages("classInt")}
  library(classInt)
if(!require("cowplot")){
  install.packages("cowplot")}
  library(cowplot)
if(!require("beeswarm")){
  install.packages("beeswarm")}
  library(beeswarm)
if(!require("rworldmap")){
  install.packages("rworldmap")}
  library(rworldmap)
if(!require("RColorBrewer")){
  install.packages("RColorBrewer")}
  library(RColorBrewer)
if(!require("cowplot")){
  install.packages("cowplot")}
  library(cowplot)
if(!require("ldatuning")){
  install.packages("ldatuning")}
  library(ldatuning)
if(!require("SnowballC")){
  install.packages("SnowballC")}
  library(SnowballC)


source('../lib/tf_idf.R')
source('../lib/lda.R')

```

# Step 2: Read data from working directory.

The original data file is approximately 28 MB in compressed state; therefore we download data locally and read it from directory rather than reading it from the “data” folder. Link to download zip file in located in the README.md file in the data folder of the repository. Alternatively, one can skip **Step 2** and **Step 3** and proceed with cleaned and transformed dataset from **Step 4**.

```{r warning=FALSE, message=FALSE}

path  = "C:/Users/heldo/Desktop/analysisData.csv"
data = read.csv(path, header = TRUE)

```


# Step 3: Feature generation, data selection.

As we are specifically interested in understanding of what makes a successful and unsuccessful listing write up, it is not very efficient to look at all 29,142 observations from the dataset. To extract useful insights, we decided to get rid of listings with "average performance" and come up with two polar samples that present "the best" and "the worst" listings (further defined as "Good" and "Bad"). To measure "goodness" of the listing we created a composite indicator that considers score ratings, two measures of popularity (overall number of reviews and number of last month's reviews), and "super host indicator". To reduce bias in the sample, we controlled for location (simultaneously limiting it to most popular options) and price bucket.

```{r message=FALSE, warning=FALSE}
#EDA needes to obtain stratified sample
hist(data$price, breaks = 100) #distribution of prices help to determine price buckets
```
From this right-skewed disctribution it is interesting to observe that long tail of very expensive apartments.
```{r}
hist(data$reviews_per_month, breaks = 100, xlim = c(0, 10)) #distribution of reviews per month helps set threshold for this indicator
```
This is another right-skewed distribution; Number of Rewies per month above two can be considered as substantially above average, so we choose it as an upper threshold. 

```{r}
plot(data$neighbourhood_group_cleansed) # Brooklyn and Manhattan are reasonable choices to limit to when placing controls over location. 
```
Based on distribution above and intention to control for location and price bucket, we construct the sample to analyse below.

```{r message=FALSE, warning=FALSE}

 # create price brackets #
data$price_bkt <- as.numeric(cut2(data$price, g=4))
 # re-create buroughs field #
data$location_bkt <- data$neighbourhood_group_cleansed
 # create good locations and bad locations based on paramaters #
polar_data <- data %>%
          group_by(id) %>%
          mutate(category = case_when(
                          review_scores_rating >= 87 &
                            reviews_per_month  >= 2 &
                            host_is_superhost  == 't' ~ 'Good',
                          review_scores_rating <= 87 &
                            reviews_per_month  <= 0.5 &
                            host_is_superhost  == 'f' ~ 'Bad')) %>%
          filter(category %in% c('Good', 'Bad')) %>%
          filter(location_bkt %in% c('Manhattan', 'Brooklyn')) %>%
          mutate(strat = case_when(
                        location_bkt == 'Manhattan' & price_bkt == 1 ~ 1,#1 is cheapest 4 is most expensive
                        location_bkt == 'Manhattan' & price_bkt == 2 ~ 2,
                        location_bkt == 'Manhattan' & price_bkt == 3 ~ 3,
                        location_bkt == 'Manhattan' & price_bkt == 4 ~ 4,
                        location_bkt == 'Brooklyn' & price_bkt == 1 ~ 5,
                        location_bkt == 'Brooklyn' & price_bkt == 2 ~ 6,
                        location_bkt == 'Brooklyn' & price_bkt == 3 ~ 7,
                        location_bkt == 'Brooklyn' & price_bkt == 4 ~ 8))

save(polar_data, file = "../output/stratified_sample.Rdata")

 # stratification outline #
table(polar_data$strat, polar_data$category)
```

# Step 4: Alalysis. 

```{r}

load("../output/stratified_sample.Rdata") #polar_data

```

## 4.1 Length of the textual variables.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}

# calculate the length 
polar_data = polar_data %>% mutate(total_length = nchar(as.character(description))+nchar(as.character(summary))+nchar(as.character(space))+nchar(as.character(neighborhood_overview))+nchar(as.character(notes))+nchar(as.character(transit))+nchar(as.character(access))+nchar(as.character(interaction))+nchar(as.character(house_rules))+nchar(as.character(host_about)))
 # plot #
 ggplot(polar_data, aes(total_length, fill = category)) + geom_histogram() + facet_grid(rows = vars(strat), cols = vars(category)) + ggtitle("Total Lengths of  Listings in Good and Bad Category") + theme(axis.text.x = element_text(size=8),axis.text.y = element_text(size=5))
 
 #ggsave("../figs/total_length_category.png")
 
```

On average, successful listings are longer and it is consistent throughout 8 strata of sample. To be more precise, we can compare means.

```{r}

polar_data %>%
  select(id, category, total_length) %>%
  group_by(category) %>%
  summarise(mean = mean(total_length))

```
So, first free tip for a new host is to invest energy to write more text to describe the apartment. 


```{r}
  # calculate lengths as variables
 polar_data_full = polar_data %>% 
 mutate(description_length = nchar(as.character(description))) %>%
 mutate(space_length = nchar(as.character(space))) %>%
 mutate(neighborhood_overview_length = nchar(as.character(neighborhood_overview))) %>%
 mutate(transit_length = nchar(as.character(transit))) %>%
 mutate(access_length = nchar(as.character(access))) %>%
 mutate(interaction_length = nchar(as.character(interaction))) %>%
 mutate(house_rules_length = nchar(as.character(house_rules))) %>%
 mutate(host_length = nchar(as.character(host_about)))

 # plot

plt1 = ggplot(polar_data_full, aes(description_length, fill = category)) + geom_density(alpha = 0.3) + theme(axis.text.x = element_blank(), axis.text.y = element_blank(),axis.ticks = element_blank(),axis.title.x= element_blank(), legend.position = "none")
plt2 = ggplot(polar_data_full, aes(space_length, fill = category)) + geom_density(alpha = 0.3) + theme(axis.text.x = element_blank(), axis.text.y = element_blank(),axis.ticks = element_blank(),axis.title.x= element_blank(),axis.title.y= element_blank(), legend.position = "none")
plt3 = ggplot(polar_data_full, aes(neighborhood_overview_length, fill = category)) + geom_density(alpha = 0.3) + theme(axis.text.x = element_blank(), axis.text.y = element_blank(),axis.ticks = element_blank(), axis.title.x= element_blank(),axis.title.y= element_blank(),legend.position = "none")
plt4 = ggplot(polar_data_full, aes(transit_length, fill = category)) + geom_density(alpha = 0.3) + theme(axis.text.x = element_blank(), axis.text.y = element_blank(),axis.ticks = element_blank(), axis.title.x= element_blank(),legend.position = "none")
plt5 = ggplot(polar_data_full, aes(access_length, fill = category)) + geom_density(alpha = 0.3) + theme(axis.text.x = element_blank(), axis.text.y = element_blank(),axis.ticks = element_blank(), axis.title.x= element_blank(),axis.title.y= element_blank(),legend.position = "none")
plt6 = ggplot(polar_data_full, aes(interaction_length, fill = category)) + geom_density(alpha = 0.3) + theme(axis.text.x = element_blank(), axis.text.y = element_blank(),axis.ticks = element_blank(), axis.title.x= element_blank(),axis.title.y= element_blank(),legend.position = "none")
plt7 = ggplot(polar_data_full, aes(house_rules_length, fill = category)) + geom_density(alpha = 0.3) + theme(axis.text.x = element_blank(), axis.text.y = element_blank(),axis.ticks = element_blank(),axis.title.x= element_blank(), legend.position = "none")
plt8 = ggplot(polar_data_full, aes(host_length, fill = category)) + geom_density(alpha = 0.3) + theme(axis.text.x = element_blank(), axis.text.y = element_blank(),axis.title.x= element_blank(), axis.title.y= element_blank(),axis.ticks = element_blank())


plot_grid(plt1, plt2, plt3, plt4, plt5, plt6, plt7, plt8, labels = c("Description", "Space", "Neighbourhood", "Transit", "Access", "Interaction", "Rules", "Host"), label_size = 8, label_x = 0.4, scale = 1.1)
#ggsave("../figs/density_length_category.png")
 
```
Observing how much "good" and "bad" hosts write about their listings, we observe same the general trend: successful hosts are more loquacious. It seems that unseccessful hosts underutilise the opportunity to describe their apartment, especially when it comes to - most probably optional - "space", "neighborhood", "transit", "access", "interaction" and "rules" categories. Interestingly, both "good" and "bad" hosts often ignore description of themselves, where we observe very similar disctribution shapes. Another interesting observation is about mandatory field "description": variance is pretty low for "good" listings, especially, comparing to high variance of "bad" listings.
```{r}

ggplot(polar_data_full, aes(category, description_length, fill = category)) + geom_boxplot() 
#ggsave("../figs/description_length_boxplot.png")

```

The boxplot provides clear guidance on how to approach this particular mandatory category: to be successful one should make an effort to write 1000 words. 

```{r message=FALSE, warning=FALSE, paged.print=FALSE}

polar_data_full = polar_data_full %>% 
  mutate(full_text = paste(as.character(description), as.character(summary),as.character(space), as.character(neighborhood_overview),as.character(notes),as.character(transit),as.character(access),as.character(interaction),as.character(house_rules),as.character(host_about)))%>% 
  mutate(num_exclam_interaction = str_count(full_text, "!"))

ggplot(polar_data_full, aes(num_exclam_interaction, fill = category)) + geom_density(alpha = 0.3) + theme(axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank()) + scale_x_log10() + ggtitle("A Quick Note about Exclamation Marks")

#ggsave("../figs/density_exclamation_marks.png")

```

We observe slightly higher tendency to add emotion to the listing utilizing most straightforward tool - exclamation marks in the cohort of "good" hosts. However, the difference between groups is insignificant. So, there is no evidence that exclamation marks signal successful or unsuccessful listing. So, not tip here other than keeping it reasonable. 

## 4.2 LDA topic modeling contrasting the two categories.

It is interesting to what extent narrative differs for successful and unsucessful listings. We have to treat "good" and "bad" listings separately as we compose a document-term matrix and derive insightful topics. 

First, we look into topics of rather unsuccessful narratives.

```{r}
polar_data_full_bad = polar_data_full %>% filter(category == "Bad")

polar_data_full_bad_text = paste(unlist(polar_data_full_bad$full_text), collapse =" ")
```

```{r}

corpus_bad = getCorpus(polar_data_full_bad_text)
tm_map(corpus_bad, stemDocument)
dtm_bad = DocumentTermMatrix(corpus_bad)
stat_best_bad = FindTopicsNumber(dtm_bad, topics = seq(from = 2, to = 15, by = 1),metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"), method = "Gibbs", control = list(seed = 2019), mc.cores = 2L, verbose = TRUE)

save(stat_best_bad, file = "../output/LDA_topic_number_bad.Rdata")

FindTopicsNumber_plot(stat_best_bad)
#ggsave("../figs/best_number_topics_bad.png")
```

For "bad" listings optimal number of topics is 3. 

```{r message=FALSE, warning=FALSE, paged.print=FALSE}

top_terms_by_topic_LDA(polar_data_full_bad_text, plot = T, number_of_topics = 3)
#ggsave("../figs/lda_bad.png")

```
With a share of speculation, we can summarize least favorable topics: (1) featuring Williamsburg, Brooklyn with emphasis on size of apartment, proximity to the station, restaurants and coffeshops; (2) featuring Manhattan, with emphasis on kitchen and proximity to the city and the park (indeed, last popular thing on Manhattan is to cook); (3) walking distance to bars and shops also does not signal a successful description.

Now, we do the same exercise for "good" listings.
```{r}
polar_data_full_good = polar_data_full %>% filter(category == "Good")

polar_data_full_good_text = paste(unlist(polar_data_full_good$full_text), collapse =" ")
```

```{r}
 
corpus_good = getCorpus(polar_data_full_good_text)
tm_map(corpus_good, stemDocument)
dtm_good = DocumentTermMatrix(corpus_good)
stat_best_good = FindTopicsNumber(dtm_good, topics = seq(from = 2, to = 15, by = 1),metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"), method = "Gibbs", control = list(seed = 2019), mc.cores = 2L, verbose = TRUE)

save(stat_best_good, file = "../output/LDA_topic_number_good.Rdata")

FindTopicsNumber_plot(stat_best_good)
#ggsave("../figs/best_number_topics_good.png")
```
For "good" listings optimal number of topics is also 3. General behavior of the graphs is similar. It just can be noted that Griffiths's method proposes higher topic variability for "good" hosts. 

```{r message=FALSE, warning=FALSE, paged.print=FALSE}

top_terms_by_topic_LDA(polar_data_full_good_text, plot = T, number_of_topics = 3)
#ggsave("../figs/lda_good.png")

```

Here, summary fo the topics may be the following: (1) combination on space, floor and convinient logistics; (2) apartment for family with storage space, featuring modern look and bathroom; (3) a fusion from "bad" scenarios: Manhattan and Brooklyn neighborhoods, restaurants and kitchen, as well as logistical benefits. Especially in he last topic, there is a noticable overlap between narratives from "good" and "bad" listings. We paid attention to several distinct features found only in "good" lsitings: 
* reference to how guests "feel";
* reference to "family"
* openness to "questions".

"Bathroom" is a leading word for a "good topic", mentioning "kitchen" is controversial. 

```{r}
#!!! MAX, I didnt touch it, please, decive what you want to do with it. renamed the df as it interferes with further flow
 # integrate the topics into the base dataset #
  #corpus_mk = getCorpus(polar_data_full$full_text)
  #DTM_mk = DocumentTermMatrix(corpus_mk)
  
  #unique_indexes_mk = unique(DTM_mk$i) 
  #DTM_mk = DTM_mk[unique_indexes_mk,]
  
  #lda = LDA(DTM_mk, k = 4, method = "Gibbs", control = list(iter = 2000,  thin = 400, nstart = 5, best = T, seed = list(1234, 423, 2211, #1122, 3345)))

#topics_words <- tidy(lda, matrix = 'beta')
##topics_docs <- tidy(lda, matrix = 'gamma')
#topics_docs <- topics_docs %>% spread(topic, gamma)
#topics_docs$document <- as.integer(topics_docs$document)
#polar_data_full_1 <- polar_data_full %>% mutate(tag = row_number())
#polar_data_full_1 <- polar_data_full_1 %>% inner_join(topics_docs, by = c('tag' = 'document'))
#common_topic <- as.data.frame(t(apply(polar_data_full_1[,112:115], 1, function(x) as.numeric(x == max(x)))))
#polar_data_full_1 <- polar_data_full_1 %>% cbind(common_topic)

```

## 4.2 TD_IDF Analysis of Categories.

Analysis is performed on 'summary' data.

```{r message=FALSE, warning=FALSE}
polar_data_full$summary = as.character(
polar_data_full$summary)
polar_data_full_summary = polar_data_full%>% select(summary, category)

top_terms_by_topic_tfidf(polar_data_full_summary, summary, category, plot = T)

ggsave("../figs/td_idf_good_summary.png")

```
Next tip from term frequency/inverse document frequency analysis: for a successful write up use standard keyboard, practice proper English language, mention social clues (such as membership and charity) and fancy notions (such as "manor" instead of, say, "house"). 


Step 08: Polarity function (we have a missing function somewhere in here)

```{r}
# MK: I still cannot rrun the polarity functions #
polar_data_full_summary %>%
group_by(category) %$% polarity(summary)
```

```{r}
polarity_summary  = polar_data_full_summary %>%
group_by(category) %$% polarity(summary, category)
polarity_summary
```

```{r}
plot(polarity_summary)
scores(polarity_summary)
counts(polarity_summary)
```

Step 09: Sentiment analysis with emotions.

```{r message=FALSE, warning=FALSE}
get.dtm <- function(text.data, unique.only = F){
  corpus = VCorpus(VectorSource(text.data))
  dtm = DocumentTermMatrix(corpus, control = list(
    tolower = TRUE,
    removeNumbers = TRUE,#we aren't concerned with informativeness, so few clearning options
    stopwords = TRUE,
    removeNumbers = TRUE, 
    removePunctuation = TRUE,
    stripWhitespace = TRUE))
  if (unique.only == T){
    non_zero_entries = unique(DTM$i) #omits zero entries
    dtm = dtm[non_zero_entries,]
  }
  return(dtm)
}

good = polar_data_full_summary %>% filter(category == "Good")
bad = polar_data_full_summary %>% filter(category == "Bad")


#tm_map(gt, stemDocument)
dtm_good <- get.dtm(good$summary)
dtm_matrix_good = as.matrix(dtm_good)
dim(dtm_matrix_good)

dtm_bad <- get.dtm(bad$summary)
dtm_matrix_bad = as.matrix(dtm_bad)
dim(dtm_matrix_bad)

tidy_format_good = tidy(dtm_good)
tidy_format_good

tidy_format_bad = tidy(dtm_bad)
tidy_format_bad
```

```{r}
#sentiments dataframe from tidytext package
lexicon = get_sentiments('afinn')
lexicon %>% count(score)
```
```{r}
lexicon2 = get_sentiments('nrc')#bing, loughran
lexicon2_counts = lexicon2 %>% count(sentiment)
lexicon2_counts
```

```{r}
ggplot(lexicon2_counts, aes(sentiment, n)) + geom_col() 
```
```{r}
lexicon3 = get_sentiments('bing')
tidy_bing_words_good = inner_join(tidy_format_good, lexicon3, by = c("term" = "word"))
tidy_bing_words_good = tidy_bing_words_good %>% mutate(index = as.numeric(document))
tidy_bing_words_good


tidy_bing_words_bad = inner_join(tidy_format_bad, lexicon3, by = c("term" = "word"))
tidy_bing_words_bad = tidy_bing_words_bad %>% mutate(index = as.numeric(document))
tidy_bing_words_bad

```
```{r}
tidy_bing_words_good_count = tidy_bing_words_good %>%count(sentiment, index)
tidy_bing_words_good_count

tidy_bing_words_bad_count = tidy_bing_words_bad %>%count(sentiment, index)
tidy_bing_words_bad_count
```
```{r}
tidy_bing_words_good_spread = tidy_bing_words_good_count %>% spread(sentiment, n, fill = 0)
tidy_bing_words_good_spread

tidy_bing_words_bad_spread = tidy_bing_words_bad_count %>% spread(sentiment, n, fill = 0)
tidy_bing_words_bad_spread
```
Results are skewed more positvely so difficult to infer.

```{r}
data(sentiments)
afinn = subset(sentiments, sentiments$lexicon == "AFINN") #one scientist labeling
head(afinn)
```
```{r}
nrc = subset(sentiments, sentiments$lexicon == 'nrc')
head(nrc,30)
```


```{r}
affin_join_good = inner_join(tidy_format_good, afinn, by = c("term" = "word"))
affin_join_good[1:10,]

affin_join_bad = inner_join(tidy_format_bad, afinn, by = c("term" = "word"))
affin_join_bad[1:10,]

affin_join_good = affin_join_good %>% mutate(category = "good")
affin_join_bad = affin_join_bad %>% mutate(category = "bad")

affin_join = rbind(affin_join_good, affin_join_bad)

```
```{r}
grouped_affin = affin_join %>% 
  group_by(category, document) %>%
  summarize(total_score  = sum(score)) %>%
  arrange(total_score)
```
```{r}
ggplot(grouped_affin, aes(document, total_score, col = category)) + geom_point()
```
Distribution is normal.


```{r}
plutchik_good = tidy_format_good %>% 
  inner_join(nrc, by = c("term" = "word")) %>% 
  filter(!sentiment %in% c("positive", "negative")) %>%
  group_by(sentiment) %>%
  summarize(total_count = sum(count))

ggplot(plutchik_good, aes(sentiment, total_count, fill = sentiment)) + geom_bar(stat = "identity")
```

```{r}
plutchik_bad = tidy_format_bad %>% 
  inner_join(nrc, by = c("term" = "word")) %>% 
  filter(!sentiment %in% c("positive", "negative")) %>%
  group_by(sentiment) %>%
  summarize(total_count = sum(count))

ggplot(plutchik_bad, aes(sentiment, total_count, fill = sentiment)) + geom_bar(stat = "identity")
```
#no insights

#redo as I added host since
```{r message=FALSE, warning=FALSE}
good = polar_data %>% filter(category == "Good")
bad = polar_data %>% filter(category == "Bad")

dtm_good <- get.dtm(good$summary)
dtm_matrix_good = as.matrix(dtm_good)
dim(dtm_matrix_good)

dtm_bad <- get.dtm(bad$summary)
dtm_matrix_bad = as.matrix(dtm_bad)
dim(dtm_matrix_bad)

tidy_format_good = tidy(dtm_good)
tidy_format_good

tidy_format_bad = tidy(dtm_bad)
tidy_format_bad



plutchik2_good = tidy_format_good %>% 
  inner_join(nrc, by = c("term" = "word")) %>% 
  filter(!sentiment %in% c("positive", "negative")) %>%
  group_by(sentiment) #%>%
  #mutate(host_since = inner_join(test_df$host_since, by = "document"))
plutchik2_good$document = as.numeric(plutchik2_good$document)
date_plutchik2_good = inner_join(plutchik2_good,good, by = "document") %>%
  group_by(document,host_since, sentiment) %>%
  summarize(total_count = sum(count))
  
  
  
ggplot(date_plutchik2_good, aes(date(host_since), total_count)) + geom_smooth() + facet_wrap(~sentiment) 

```

```{r message=FALSE, warning=FALSE}
test_df = data %>% select(id, summary, price, review_scores_rating, host_since) %>% head(200)
test_df <- cbind(document = as.numeric(rownames(test_df)), test_df)
plutchik2 = tidy_format_good %>% 
  inner_join(nrc, by = c("term" = "word")) %>% 
  filter(!sentiment %in% c("positive", "negative")) %>%
  group_by(sentiment) #%>%
  #mutate(host_since = inner_join(test_df$host_since, by = "document"))
plutchik2$document = as.numeric(plutchik2$document)
date_plutchik2 = inner_join(plutchik2,test_df, by = "document") %>%
  group_by(document,host_since, sentiment) %>%
  summarize(total_count = sum(count))
  
  
  
ggplot(date_plutchik2, aes(date(host_since), total_count)) + geom_smooth() + facet_wrap(~sentiment) 

```
Not sure how valuable this is as it is not really timeseries, just the date the host started on airbnb...

```{r message=FALSE, warning=FALSE}
test_df = data %>% select(id, summary, price, review_scores_rating, host_since) %>% head(200)
test_df <- cbind(document = as.numeric(rownames(test_df)), test_df)
plutchik2 = tidy_format_bad %>% 
  inner_join(nrc, by = c("term" = "word")) %>% 
  filter(!sentiment %in% c("positive", "negative")) %>%
  group_by(sentiment) #%>%
  #mutate(host_since = inner_join(test_df$host_since, by = "document"))
plutchik2$document = as.numeric(plutchik2$document)
date_plutchik2 = inner_join(plutchik2,test_df, by = "document") %>%
  group_by(document,host_since, sentiment) %>%
  summarize(total_count = sum(count))
  
  
  
ggplot(date_plutchik2, aes(date(host_since), total_count)) + geom_smooth() + facet_wrap(~sentiment) 

```
smth might be interesting about those trends. Maybe the anticipation slot because it is a more popular emotion in the BAD category.


```{r}
#good
bind_pos_neg_polarity  = tidy_format_good %>%
  inner_join(lexicon3, by = c("term" = "word")) %>%
  count(document, term, sentiment, wt = count) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(polarity = positive - negative)
  
  bing_polarity_doc  = bind_pos_neg_polarity %>%
  group_by(document) %>%
  summarize(summary_polarity = sum(polarity)) %>%
  arrange(summary_polarity)
hist(bing_polarity_doc$summary_polarity)

```
```{r}
#good
bind_pos_neg_polarity = bind_pos_neg_polarity%>% 
  mutate(positive_negative = ifelse(polarity > 0, "positive", "negative"))

ggplot(bind_pos_neg_polarity, aes(reorder(term, polarity), polarity, fill = positive_negative)) + geom_col() + ggtitle("Airbnb Summaries Polarity")  +theme(axis.text.x = element_text(angle = 90, vjust = -0.1, size = 2))

```

```{r}
#good
pol_subsections = function(df){
  x.pos = subset(df$text, df$polarity > 0)
  x.neg = subset(df$text, df$polarity < 0)
  x.pos = paste(x.pos, collapse = " ")
  x.neg = paste(x.neg, collapse = " ")
  all.terms = c(x.pos, x.neg)
  return(all.terms)
  
}

text_polarity = counts(polarity_summary) 
text_polarity = text_polarity %>% select( id  = id, polarity = polarity,text = text.var)
all_terms = pol_subsections(text_polarity) 

all_corpus = all_terms %>% VectorSource() %>% VCorpus()

all_tdm = TermDocumentMatrix(all_corpus, control = list(removePunctuation = TRUE, stopwords = stopwords(kind = "en"))) %>%
  as.matrix() %>%
  set_colnames(c('positive', 'negative'))
#smth wrong here
comparison.cloud(all_tdm, max.words = 70, scale=c(2,.05), colors = c("darkgreen", "darkred"))
class(all_tdm)
```

```{r}
#bad
bind_pos_neg_polarity  = tidy_format_bad %>%
  inner_join(lexicon3, by = c("term" = "word")) %>%
  count(document, term, sentiment, wt = count) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(polarity = positive - negative)
  
  bing_polarity_doc  = bind_pos_neg_polarity %>%
  group_by(document) %>%
  summarize(summary_polarity = sum(polarity)) %>%
  arrange(summary_polarity)
hist(bing_polarity_doc$summary_polarity)

```

```{r}
#bad
bind_pos_neg_polarity = bind_pos_neg_polarity%>% 
  mutate(positive_negative = ifelse(polarity > 0, "positive", "negative"))

ggplot(bind_pos_neg_polarity, aes(reorder(term, polarity), polarity, fill = positive_negative)) + geom_col() + ggtitle("Airbnb Summaries Polarity")  +theme(axis.text.x = element_text(angle = 90, vjust = -0.1, size = 2))

```

```{r}
#bad
pol_subsections = function(df){
  x.pos = subset(df$text, df$polarity > 0)
  x.neg = subset(df$text, df$polarity < 0)
  x.pos = paste(x.pos, collapse = " ")
  x.neg = paste(x.neg, collapse = " ")
  all.terms = c(x.pos, x.neg)
  return(all.terms)
  
}

text_polarity = counts(polarity_summary) 
text_polarity = text_polarity %>% select( id  = id, polarity = polarity,text = text.var)
all_terms = pol_subsections(text_polarity) 

all_corpus = all_terms %>% VectorSource() %>% VCorpus()

all_tdm = TermDocumentMatrix(all_corpus, control = list(removePunctuation = TRUE, stopwords = stopwords(kind = "en"))) %>%
  as.matrix() %>%
  set_colnames(c('positive', 'negative'))
#smth wrong here
comparison.cloud(all_tdm, max.words = 70, scale=c(2,.05), colors = c("darkgreen", "darkred"))

```

